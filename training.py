import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import numpy as np
import cv2
from pathlib import Path
import random
from typing import List, Tuple, Optional, Dict, Any
import napari
import napari.layers
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import WandbLogger
import segmentation_models_pytorch as smp
import warnings

class NapariI3DDataset(Dataset):
    """
    Dataset for i3d model training from napari layers.
    Uses the same tiling approach as the original i3d implementation.
    """
    
    def __init__(self, 
                 viewer: napari.Viewer,
                 mode: str = 'train',
                 tile_size: int = 64,
                 large_tile_size: int = 256,
                 stride: int = 32,
                 in_chans: int = 30,
                 filter_empty: bool = True,
                 filter_threshold: float = 0.01):
        
        self.viewer = viewer
        self.mode = mode
        self.tile_size = tile_size
        self.large_tile_size = large_tile_size
        self.stride = stride
        self.in_chans = in_chans
        self.filter_empty = filter_empty
        self.filter_threshold = filter_threshold
        
        # Get transform
        self.transform = self._get_transform()
        
        # Load data from napari
        self.samples = self._load_samples()
        
    def _get_transform(self):
        """Get augmentation pipeline"""
        if self.mode == 'train':
            # Updated to use Affine instead of ShiftScaleRotate
            return A.Compose([
                A.Resize(self.tile_size, self.tile_size),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.5),
                # A.RandomBrightnessContrast(p=0.75),
                A.Affine(
                    scale=(0.9, 1.1),
                    translate_percent={'x': (-0.15, 0.15), 'y': (-0.15, 0.15)},
                    rotate=(-360, 360),
                    shear=(-5, 5),
                    p=0.75
                ),
                A.OneOf([
                    A.GaussianBlur(),
                    A.MotionBlur(),
                ], p=0.4),
                # Fixed CoarseDropout parameters
                A.CoarseDropout(
                    max_holes=2,
                    max_width=int(self.tile_size * 0.2), 
                    max_height=int(self.tile_size * 0.2),
                    fill_value=0,  # Changed from mask_fill_value
                    p=0.5
                ),
                A.Normalize(mean=[0] * self.in_chans, std=[1] * self.in_chans),
                ToTensorV2(transpose_mask=True),
            ])
        else:
            return A.Compose([
                A.Resize(self.tile_size, self.tile_size),
                A.Normalize(mean=[0] * self.in_chans, std=[1] * self.in_chans),
                ToTensorV2(transpose_mask=True),
            ])
    def _load_samples(self):
        """Load and tile data from napari viewer"""
        samples = []
        
        # Create debug directory for saving labels
        debug_dir = Path("debug_labels")
        debug_dir.mkdir(exist_ok=True)
        print(f"\n{'='*50}")
        print(f"Saving label images to {debug_dir.absolute()} for verification")
        print(f"{'='*50}\n")
        
        # Get image and label layers - make copies to avoid reference issues
        # Exclude any autogenerated sample layers (names starting with 'sample_')
        image_layers = [l for l in self.viewer.layers 
                        if isinstance(l, napari.layers.Image) and not l.name.startswith('sample_')]
        label_layers = [l for l in self.viewer.layers 
                        if isinstance(l, napari.layers.Labels) and not l.name.startswith('sample_')]
        
        # Match image-label pairs
        for img_layer in image_layers:
            # Find corresponding label
            label_layer = None
            for lbl_layer in label_layers:
                if lbl_layer.name.startswith(f"{img_layer.name}_"):
                    label_layer = lbl_layer
                    break
            
            if label_layer is None:
                continue
                
            print(f"Processing pair: {img_layer.name} -> {label_layer.name}")
            
            # Get data - make copies to avoid reference issues
            orig_image = np.array(img_layer.data)
            image = orig_image.astype(np.float32)
            # If the source image is uint16, rescale to approx uint8 range
            if orig_image.dtype == np.uint16:
                image = image / 257.0
            mask = np.array(label_layer.data, dtype=np.float32)
            np.save('img_layer.npy', image)

            print(f"  Original image shape: {image.shape}, mask shape: {mask.shape}")
            
            # Save original mask info for debugging
            mask_debug = mask.copy()
            mask_debug_info = f"shape_{mask.shape}"
            
            # FIX: Properly handle napari label dimensions
            # Napari 3D data is typically (Z, Y, X) or (Y, X) for 2D
            if mask.ndim == 3:
                # 3D mask - could be (Z, H, W) where Z might be 65 (layers)
                print(f"  3D mask detected: shape {mask.shape}")
                
                # Option 1: Take a specific slice (e.g., middle)
                # Option 2: Take max projection along Z
                # Option 3: Take sum along Z and threshold
                
                # Check which dimension is likely Z (usually the first for napari)
                if mask.shape[0] < 100 and mask.shape[1] > 100 and mask.shape[2] > 100:
                    # Likely (Z, H, W) - Z is small (e.g., 65 layers)
                    print(f"  Interpreting as (Z={mask.shape[0]}, H={mask.shape[1]}, W={mask.shape[2]})")
                    
                    # Take max projection along Z axis
                    mask = np.max(mask, axis=0)
                    mask_debug_info = f"max_proj_along_Z_from_{mask_debug.shape}"
                    
                    # Alternative: take middle slice
                    # mid_z = mask.shape[0] // 2
                    # mask = mask[mid_z]
                    # mask_debug_info = f"slice_{mid_z}_from_{mask_debug.shape}"
                    
                elif mask.shape[2] < 100 and mask.shape[0] > 100 and mask.shape[1] > 100:
                    # Possibly (H, W, Z) - less common in napari but possible
                    print(f"  Interpreting as (H={mask.shape[0]}, W={mask.shape[1]}, Z={mask.shape[2]})")
                    mask = np.max(mask, axis=2)
                    mask_debug_info = f"max_proj_along_last_axis_from_{mask_debug.shape}"
                else:
                    # Ambiguous - use max projection along smallest dimension
                    min_dim = np.argmin(mask.shape)
                    print(f"  Ambiguous shape, projecting along axis {min_dim}")
                    mask = np.max(mask, axis=min_dim)
                    mask_debug_info = f"max_proj_axis_{min_dim}_from_{mask_debug.shape}"
            
            # Ensure mask is 2D now
            if mask.ndim != 2:
                print(f"  Warning: After processing, mask still has {mask.ndim} dimensions, skipping")
                continue
                
            print(f"  Processed mask shape: {mask.shape}")
            
            # Binarize labels: any non-zero value is considered ink
            mask = (mask > 0).astype(np.float32)
            
            # Handle image dimensionality similarly
            if image.ndim == 2:
                # Single 2D slice - replicate to create volume
                print(f"  2D image, replicating to {self.in_chans} channels")
                image = np.stack([image] * self.in_chans, axis=2)
                
            elif image.ndim == 3:
                # 3D image - determine orientation
                print(f"  3D image detected: shape {image.shape}")
                
                # Check if it's (Z, H, W) like napari typically uses
                if image.shape[0] < 100 and image.shape[1] > 100 and image.shape[2] > 100:
                    # Likely (Z, H, W) - transpose to (H, W, Z)
                    print(f"  Transposing from (Z, H, W) to (H, W, Z)")
                    image = np.transpose(image, (1, 2, 0))
                    
                # Now image should be (H, W, C/Z)
                if image.shape[2] < self.in_chans:
                    padding = self.in_chans - image.shape[2]
                    print(f"  Padding {padding} channels to reach {self.in_chans}")
                    image = np.pad(image, ((0, 0), (0, 0), (0, padding)), mode='edge')
                elif image.shape[2] > self.in_chans:
                    # Take center channels
                    mid = image.shape[2] // 2
                    start = max(0, mid - self.in_chans // 2)
                    end = start + self.in_chans
                    print(f"  Extracting channels {start} to {end} from {image.shape[2]} total")
                    image = image[:, :, start:end]
            
            # Verify dimensions match
            print(f"  Final shapes - Image: {image.shape}, Mask: {mask.shape}")
            if image.shape[0] != mask.shape[0] or image.shape[1] != mask.shape[1]:
                print(f"  ERROR: Spatial dimensions don't match! Image: ({image.shape[0]}, {image.shape[1]}), Mask: ({mask.shape[0]}, {mask.shape[1]})")
                continue
            
            # Save the processed mask for debugging
            mask_save = mask.copy()
            
            # Convert to 0-255 range for saving
            if mask_save.max() <= 1.0:
                mask_save = (mask_save * 255).astype(np.uint8)
            else:
                mask_save = mask_save.astype(np.uint8)
            
            # Save mask as PNG
            mask_filename = debug_dir / f"{label_layer.name}_{mask_debug_info}.png"
            cv2.imwrite(str(mask_filename), mask_save)
            print(f"  ✓ Saved label to: {mask_filename}")
            
            # Save statistics
            stats_filename = debug_dir / f"{label_layer.name}_stats.txt"
            with open(stats_filename, 'w') as f:
                f.write(f"Label Layer: {label_layer.name}\n")
                f.write(f"Original Shape: {mask_debug.shape}\n")
                f.write(f"Processing: {mask_debug_info}\n")
                f.write(f"Processed Shape: {mask.shape}\n")
                f.write(f"Final Image Shape: {image.shape}\n")
                f.write(f"Min Value: {mask.min():.4f}\n")
                f.write(f"Max Value: {mask.max():.4f}\n")
                f.write(f"Mean Value: {mask.mean():.4f}\n")
                f.write(f"Non-zero pixels: {np.count_nonzero(mask)}\n")
                f.write(f"Non-zero percentage: {100 * np.count_nonzero(mask) / mask.size:.2f}%\n")
            print(f"  ✓ Saved stats to: {stats_filename}")
            
            # Create colored visualization
            if np.any(mask > 0):
                mask_viz = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)
                mask_normalized = (mask / mask.max() * 255).astype(np.uint8) if mask.max() > 0 else mask
                mask_viz[:, :, 1] = mask_normalized  # Green for ink
                mask_viz[:, :, 0] = 100  # Dark red background
                
                viz_filename = debug_dir / f"{label_layer.name}_visualization.png"
                cv2.imwrite(str(viz_filename), mask_viz)
                print(f"  ✓ Saved visualization to: {viz_filename}")
            
            # Also save first few slices if original was 3D
            if mask_debug.ndim == 3 and mask_debug.shape[0] > 1:
                for i in range(min(5, mask_debug.shape[0])):
                    slice_save = mask_debug[i]
                    if slice_save.max() <= 1.0:
                        slice_save = (slice_save * 255).astype(np.uint8)
                    else:
                        slice_save = slice_save.astype(np.uint8)
                    slice_filename = debug_dir / f"{label_layer.name}_slice_{i:03d}.png"
                    cv2.imwrite(str(slice_filename), slice_save)
                print(f"  ✓ Saved first {min(5, mask_debug.shape[0])} slices for inspection")
            
            # Continue with rest of processing...
            print(f"  Normalizing and padding...")
            
            # Normalize mask to [0, 1]
            # Now guaranteed binary {0,1}
            
            # Pad to tile size - both image and mask should be 2D spatial now
            pad0 = (self.large_tile_size - image.shape[0] % self.large_tile_size) % self.large_tile_size
            pad1 = (self.large_tile_size - image.shape[1] % self.large_tile_size) % self.large_tile_size
            
            image = np.pad(image, ((0, pad0), (0, pad1), (0, 0)), constant_values=0)
            mask = np.pad(mask, ((0, pad0), (0, pad1)), constant_values=0)
            
            # Clip image values to full uint8 dynamic range after rescaling
            image = np.clip(image, 0, 255)
            
            # Generate large tiles
            x1_list = list(range(0, image.shape[1] - self.large_tile_size + 1, self.stride))
            y1_list = list(range(0, image.shape[0] - self.large_tile_size + 1, self.stride))
            
            # Ensure we have at least one tile
            if len(x1_list) == 0:
                x1_list = [0]
            if len(y1_list) == 0:
                y1_list = [0]
            
            tile_count = 0
            non_empty_tiles = 0
            
            for y1 in y1_list:
                for x1 in x1_list:
                    y2 = min(y1 + self.large_tile_size, image.shape[0])
                    x2 = min(x1 + self.large_tile_size, image.shape[1])
                    
                    large_tile_mask = mask[y1:y2, x1:x2]
                    
                    # Track statistics
                    has_ink = np.any(large_tile_mask >= self.filter_threshold)
                    if has_ink:
                        non_empty_tiles += 1
                    
                    # Filter empty tiles in training
                    if self.mode == 'train' and self.filter_empty:
                        if np.all(large_tile_mask < self.filter_threshold):
                            continue
                    
                    # Extract smaller tiles from large tile
                    for sub_y in range(0, y2-y1-self.tile_size+1, self.tile_size):
                        for sub_x in range(0, x2-x1-self.tile_size+1, self.tile_size):
                            tile_y1 = y1 + sub_y
                            tile_x1 = x1 + sub_x
                            tile_y2 = tile_y1 + self.tile_size
                            tile_x2 = tile_x1 + self.tile_size
                            
                            if tile_y2 > image.shape[0] or tile_x2 > image.shape[1]:
                                continue
                                
                            tile_image = image[tile_y1:tile_y2, tile_x1:tile_x2]
                            tile_mask = mask[tile_y1:tile_y2, tile_x1:tile_x2]
                            
                            # Verify tile dimensions
                            if tile_image.shape != (self.tile_size, self.tile_size, self.in_chans):
                                continue
                            if tile_mask.shape != (self.tile_size, self.tile_size):
                                continue
                            
                            samples.append({
                                'image': tile_image.copy(),
                                'mask': tile_mask.copy(),
                                'coords': (tile_x1, tile_y1, tile_x2, tile_y2)
                            })
                            tile_count += 1
            
            print(f"  Generated {tile_count} tiles ({non_empty_tiles} large tiles with ink)")
        
        # Save summary
        summary_filename = debug_dir / "processing_summary.txt"
        with open(summary_filename, 'w') as f:
            f.write(f"Label Processing Summary\n")
            f.write(f"{'='*50}\n")
            f.write(f"Mode: {self.mode}\n")
            f.write(f"Tile Size: {self.tile_size}\n")
            f.write(f"Large Tile Size: {self.large_tile_size}\n")
            f.write(f"Stride: {self.stride}\n")
            f.write(f"Input Channels: {self.in_chans}\n")
            f.write(f"Filter Empty: {self.filter_empty}\n")
            f.write(f"Filter Threshold: {self.filter_threshold}\n")
            f.write(f"Total Samples: {len(samples)}\n")
            f.write(f"\nIMPORTANT: Check saved images to verify:\n")
            f.write(f"1. Labels have correct spatial dimensions\n")
            f.write(f"2. Ink regions are where expected\n")
            f.write(f"3. Max projection or slicing worked correctly\n")
        
        print(f"\n{'='*50}")
        print(f"Total samples loaded for {self.mode}: {len(samples)}")
        print(f"Check {debug_dir.absolute()} for saved label images")
        print(f"{'='*50}\n")
        
        if len(samples) == 0:
            raise RuntimeError("No valid samples found. Please add real image and label layers (labels named as {image_name}_...).")
        
        return samples
    # def _load_samples(self):
    #     """Load and tile data from napari viewer"""
    #     samples = []
        
    #     # Get image and label layers - make copies to avoid reference issues
    #     image_layers = [l for l in self.viewer.layers if isinstance(l, napari.layers.Image)]
    #     label_layers = [l for l in self.viewer.layers if isinstance(l, napari.layers.Labels)]
        
    #     # Match image-label pairs
    #     for img_layer in image_layers:
    #         # Find corresponding label
    #         label_layer = None
    #         for lbl_layer in label_layers:
    #             if lbl_layer.name.startswith(f"{img_layer.name}_"):
    #                 label_layer = lbl_layer
    #                 break
            
    #         if label_layer is None:
    #             continue
                
    #         print(f"Processing pair: {img_layer.name} -> {label_layer.name}")
            
    #         # Get data - make copies to avoid reference issues
    #         image = np.array(img_layer.data, dtype=np.float32)
    #         mask = np.array(label_layer.data, dtype=np.float32)
            
    #         print(f"  Original image shape: {image.shape}, mask shape: {mask.shape}")
            
    #         # Handle mask dimensionality
    #         if mask.ndim == 3:
    #             # If mask is 3D, take the maximum projection or middle slice
    #             if mask.shape[0] == image.shape[0] and mask.shape[1] == image.shape[1]:
    #                 # Mask channels are last dimension, take max projection
    #                 mask = np.max(mask, axis=2)
    #             else:
    #                 # Mask slices are first dimension, take middle slice or max projection
    #                 if mask.shape[0] > 1:
    #                     # Take middle slice
    #                     mid_slice = mask.shape[0] // 2
    #                     mask = mask[mid_slice]
    #                 else:
    #                     mask = mask[0]
            
    #         # Ensure mask is 2D now
    #         if mask.ndim != 2:
    #             print(f"  Warning: Unexpected mask dimensions {mask.shape}, skipping")
    #             continue
            
    #         # Handle image dimensionality
    #         if image.ndim == 2:
    #             # Replicate 2D image to create volume
    #             image = np.stack([image] * self.in_chans, axis=2)
    #         elif image.ndim == 3:
    #             # Check if it's (C, H, W) or (H, W, C)
    #             if image.shape[0] == self.in_chans or (image.shape[0] < 10 and image.shape[2] > 10):
    #                 # Likely (C, H, W), transpose to (H, W, C)
    #                 image = np.transpose(image, (1, 2, 0))
                
    #             # Ensure we have enough channels
    #             if image.shape[2] < self.in_chans:
    #                 padding = self.in_chans - image.shape[2]
    #                 image = np.pad(image, ((0, 0), (0, 0), (0, padding)), mode='edge')
    #             elif image.shape[2] > self.in_chans:
    #                 # Take center channels
    #                 mid = image.shape[2] // 2
    #                 start = max(0, mid - self.in_chans // 2)
    #                 end = start + self.in_chans
    #                 image = image[:, :, start:end]
    #         else:
    #             print(f"  Warning: Unexpected image dimensions {image.shape}, skipping")
    #             continue
            
    #         print(f"  Processed image shape: {image.shape}, mask shape: {mask.shape}")
            
    #         # Normalize mask to [0, 1]
    #         if mask.max() > 1:
    #             mask = mask / 255.0
            
    #         # Pad to tile size - both image and mask should be 2D spatial now
    #         pad0 = (self.large_tile_size - image.shape[0] % self.large_tile_size) % self.large_tile_size
    #         pad1 = (self.large_tile_size - image.shape[1] % self.large_tile_size) % self.large_tile_size
            
    #         image = np.pad(image, ((0, pad0), (0, pad1), (0, 0)), constant_values=0)
    #         mask = np.pad(mask, ((0, pad0), (0, pad1)), constant_values=0)
            
    #         # Clip image values
    #         image = np.clip(image, 0, 200)
            
    #         # Generate large tiles
    #         x1_list = list(range(0, image.shape[1] - self.large_tile_size + 1, self.stride))
    #         y1_list = list(range(0, image.shape[0] - self.large_tile_size + 1, self.stride))
            
    #         # Ensure we have at least one tile
    #         if len(x1_list) == 0:
    #             x1_list = [0]
    #         if len(y1_list) == 0:
    #             y1_list = [0]
            
    #         tile_count = 0
    #         for y1 in y1_list:
    #             for x1 in x1_list:
    #                 y2 = min(y1 + self.large_tile_size, image.shape[0])
    #                 x2 = min(x1 + self.large_tile_size, image.shape[1])
                    
    #                 large_tile_mask = mask[y1:y2, x1:x2]
                    
    #                 # Filter empty tiles in training
    #                 if self.mode == 'train' and self.filter_empty:
    #                     if np.all(large_tile_mask < self.filter_threshold):
    #                         continue
                    
    #                 # Extract smaller tiles from large tile
    #                 for sub_y in range(0, y2-y1-self.tile_size+1, self.tile_size):
    #                     for sub_x in range(0, x2-x1-self.tile_size+1, self.tile_size):
    #                         tile_y1 = y1 + sub_y
    #                         tile_x1 = x1 + sub_x
    #                         tile_y2 = tile_y1 + self.tile_size
    #                         tile_x2 = tile_x1 + self.tile_size
                            
    #                         if tile_y2 > image.shape[0] or tile_x2 > image.shape[1]:
    #                             continue
                                
    #                         tile_image = image[tile_y1:tile_y2, tile_x1:tile_x2]
    #                         tile_mask = mask[tile_y1:tile_y2, tile_x1:tile_x2]
                            
    #                         # Verify tile dimensions
    #                         if tile_image.shape != (self.tile_size, self.tile_size, self.in_chans):
    #                             continue
    #                         if tile_mask.shape != (self.tile_size, self.tile_size):
    #                             continue
                            
    #                         samples.append({
    #                             'image': tile_image.copy(),  # Make copies to avoid reference issues
    #                             'mask': tile_mask.copy(),
    #                             'coords': (tile_x1, tile_y1, tile_x2, tile_y2)
    #                         })
    #                         tile_count += 1
            
    #         print(f"  Generated {tile_count} tiles from {img_layer.name}")
        
    #     print(f"Total samples loaded for {self.mode}: {len(samples)}")
        
    #     if len(samples) == 0:
    #         # Create at least one dummy sample to avoid empty dataset
    #         print("Warning: No valid samples found, creating dummy sample")
    #         dummy_image = np.random.randn(self.tile_size, self.tile_size, self.in_chans).astype(np.float32) * 100
    #         dummy_mask = np.zeros((self.tile_size, self.tile_size), dtype=np.float32)
    #         samples.append({
    #             'image': dummy_image,
    #             'mask': dummy_mask,
    #             'coords': (0, 0, self.tile_size, self.tile_size)
    #         })
        
    #     return samples
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        image = sample['image'].copy()  # Make a copy
        mask = sample['mask'].copy()
        mask = mask[:, :, None]  # Add channel dimension for transform
        
        # Apply temporal augmentation occasionally in training
        if self.mode == 'train' and random.random() < 0.3:
            image = self.fourth_augment(image)
        
        # Apply augmentation
        if self.transform:
            transformed = self.transform(image=image, mask=mask)
            image = transformed['image'].unsqueeze(0)  # Add channel dim
            mask = transformed['mask']
            
            # Downsample mask to match model output
            mask = F.interpolate(
                mask.unsqueeze(0),
                (self.tile_size // 4, self.tile_size // 4),
                mode='bilinear',
                align_corners=False
            ).squeeze(0)
        
        if self.mode == 'val':
            return image, mask, sample['coords']
        else:
            return image, mask
    
    def fourth_augment(self, image):
        """Temporal augmentation from original implementation"""
        if image.shape[2] < 20:
            return image  # Not enough channels for augmentation
            
        image_tmp = np.zeros_like(image)
        cropping_num = random.randint(18, min(26, image.shape[2]))
        
        start_idx = random.randint(0, image.shape[2] - cropping_num)
        crop_indices = np.arange(start_idx, start_idx + cropping_num)
        
        start_paste_idx = random.randint(0, image.shape[2] - cropping_num)
        
        tmp = list(range(cropping_num))
        np.random.shuffle(tmp)
        
        cutout_idx = random.randint(0, min(2, cropping_num-1))
        temporal_random_cutout_idx = tmp[:cutout_idx]
        
        image_tmp[..., start_paste_idx:start_paste_idx + cropping_num] = image[..., crop_indices]
        
        if random.random() > 0.4 and len(temporal_random_cutout_idx) > 0:
            for idx in temporal_random_cutout_idx:
                if start_paste_idx + idx < image_tmp.shape[2]:
                    image_tmp[..., start_paste_idx + idx] = 0
            
        return image_tmp


class Decoder(nn.Module):
    """Decoder from the i3d implementation"""
    def __init__(self, encoder_dims, upscale):
        super().__init__()
        self.convs = nn.ModuleList([
            nn.Sequential(
                nn.Conv2d(encoder_dims[i]+encoder_dims[i-1], encoder_dims[i-1], 3, 1, 1, bias=False),
                nn.BatchNorm2d(encoder_dims[i-1]),
                nn.ReLU(inplace=True)
            ) for i in range(1, len(encoder_dims))])

        self.logit = nn.Conv2d(encoder_dims[0], 1, 1, 1, 0)
        self.up = nn.Upsample(scale_factor=upscale, mode="bilinear", align_corners=False)

    def forward(self, feature_maps):
        for i in range(len(feature_maps)-1, 0, -1):
            f_up = F.interpolate(feature_maps[i], scale_factor=2, mode="bilinear", align_corners=False)
            f = torch.cat([feature_maps[i-1], f_up], dim=1)
            f_down = self.convs[i-1](f)
            feature_maps[i-1] = f_down

        x = self.logit(feature_maps[0])
        mask = self.up(x)
        return mask


class I3DLightningModel(pl.LightningModule):
    """PyTorch Lightning wrapper for i3d model"""
    
    def __init__(self, 
                 learning_rate: float = 2e-5,
                 in_channels: int = 1,
                 size: int = 64,
                 enc: str = 'i3d'):
        super().__init__()
        
        self.save_hyperparameters()
        self.learning_rate = learning_rate
        self.size = size
        
        # Try to import model with fallback
        try:
            from models.i3dallnl import InceptionI3d
        except ImportError:
            print("Warning: Could not import i3dallnl from models package")
            # You can add a fallback or dummy model here
            from .i3d_model import InceptionI3d  # Assuming you have a local copy
        
        # Create model backbone based on encoder type
        self.encoder_type = enc
        if self.encoder_type == 'i3d':
            self.backbone = InceptionI3d(in_channels=in_channels, num_classes=512, non_local=True)
        else:
            # resnet3d style backbone
            try:
                from models.resnetall import generate_model
            except Exception as e:
                print(f"Error importing resnet3d generator: {e}")
                raise
            # Default to depth-50 forward_features=True
            self.backbone = generate_model(model_depth=50, n_input_channels=1, forward_features=True, n_classes=1039)
        
        # Create decoder - use dummy input to get encoder dimensions
        with torch.no_grad():
            test_input = torch.rand(1, 1, 20, 256, 256)
            encoder_dims = [x.size(1) for x in self.backbone(test_input)]
        self.decoder = Decoder(encoder_dims=encoder_dims, upscale=1)
        
        # Loss functions
        self.loss_func1 = smp.losses.DiceLoss(mode='binary')
        self.loss_func2 = smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.25)
        
    def forward(self, x):
        if x.ndim == 4:
            x = x[:, None]
        feat_maps = self.backbone(x)
        feat_maps_pooled = [torch.max(f, dim=2)[0] for f in feat_maps]
        pred_mask = self.decoder(feat_maps_pooled)
        return pred_mask
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        outputs = self(x)
        loss = 0.5 * self.loss_func1(outputs, y) + 0.5 * self.loss_func2(outputs, y)
        self.log("train/loss", loss, on_step=True, on_epoch=True, prog_bar=True)
        return loss
    
    def validation_step(self, batch, batch_idx):
        x, y, coords = batch
        outputs = self(x)
        loss = 0.5 * self.loss_func1(outputs, y) + 0.5 * self.loss_func2(outputs, y)
        self.log("val/loss", loss, on_step=True, on_epoch=True, prog_bar=True)
        return loss
    
    def configure_optimizers(self):
        from torch.optim import AdamW
        optimizer = AdamW(self.parameters(), lr=self.learning_rate)
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=10, eta_min=1e-6
        )
        
        return [optimizer], [scheduler]


def train_i3d_model(viewer: napari.Viewer,
                   max_epochs: int = 10,
                   batch_size: int = 32,
                   learning_rate: float = 2e-5,
                   tile_size: int = 64,
                   in_chans: int = 30,
                   model_type: str = 'i3d',
                   checkpoint_dir: str = "./checkpoints"):
    """
    Train i3d model on napari data
    """
    
    # Create checkpoint directory if it doesn't exist
    Path(checkpoint_dir).mkdir(parents=True, exist_ok=True)
    
    # Create datasets with error handling
    try:
        train_dataset = NapariI3DDataset(
            viewer=viewer,
            mode='train',
            tile_size=tile_size,
            in_chans=in_chans
        )
        
        val_dataset = NapariI3DDataset(
            viewer=viewer,
            mode='val',
            tile_size=tile_size,
            in_chans=in_chans
        )
    except Exception as e:
        print(f"Error creating datasets: {e}")
        raise
    
    # Create dataloaders
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=16,  # Set to 0 to avoid multiprocessing issues with napari
        pin_memory=True if torch.cuda.is_available() else False,
        persistent_workers=False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=16,
        pin_memory=True if torch.cuda.is_available() else False,
        persistent_workers=False
    )
    
    # Save a quick sanity batch (first batch) for visual inspection
    try:
        debug_dir = Path("debug_labels")
        debug_dir.mkdir(exist_ok=True)

        sample_images, sample_masks = next(iter(train_loader))
        # Shapes: images -> (B, 1, C, H, W), masks -> (B, 1, H/4, W/4)
        num_to_save = min(8, sample_images.shape[0])

        for idx in range(num_to_save):
            img = sample_images[idx, 0].detach().cpu().numpy()  # (C, H, W)
            # Visualize max projection across channels
            img_max = img.max(axis=0)

            # Robust normalization to avoid all-black images
            p1, p99 = np.percentile(img_max, [1, 99])
            if p99 <= p1:
                p1, p99 = float(img_max.min()), float(img_max.max())
            denom = (p99 - p1) if (p99 - p1) > 1e-6 else 1.0
            img_norm = np.clip((img_max - p1) / denom, 0.0, 1.0)
            img_vis = (img_norm * 255.0).astype(np.uint8)

            mask_small = sample_masks[idx]  # (1, H/4, W/4)
            with torch.no_grad():
                mask_up = F.interpolate(
                    mask_small.unsqueeze(0),  # (1,1,h,w)
                    size=(tile_size, tile_size),
                    mode='bilinear',
                    align_corners=False
                ).squeeze(0).squeeze(0).detach().cpu().numpy()
            mask_vis = np.clip(mask_up * 255.0, 0, 255).astype(np.uint8)

            cv2.imwrite(str(debug_dir / f"first_batch_train_image_max_{idx:02d}.png"), img_vis)
            cv2.imwrite(str(debug_dir / f"first_batch_train_mask_upsampled_{idx:02d}.png"), mask_vis)
    except Exception as e:
        print(f"Warning: Could not save first training batch debug images: {e}")

    # Create model
    model = I3DLightningModel(
        learning_rate=learning_rate,
        size=tile_size,
        enc=model_type
    )
    model.load_state_dict(torch.load('./ted_20250807020208_0_fr_i3depoch=18.ckpt',weights_only=False)['state_dict'])
    # Create trainer
    checkpoint_callback = ModelCheckpoint(
        dirpath=checkpoint_dir,
        filename='i3d_napari_{epoch:02d}',
        monitor='train/loss',
        mode='min',
        save_top_k=10,
    )
    
    # Suppress warnings
    warnings.filterwarnings("ignore", category=UserWarning)
    
    trainer = pl.Trainer(
        max_epochs=max_epochs,
        accelerator='gpu' if torch.cuda.is_available() else 'cpu',
        devices=1,
        callbacks=[checkpoint_callback],
        precision='16-mixed' if torch.cuda.is_available() else 32,
        gradient_clip_val=1.0,
        logger=False,  # Disable default logger to reduce output
        enable_progress_bar=True,
        enable_model_summary=False
    )
    
    # Train
    print(f"Starting i3d training for {max_epochs} epochs...")
    trainer.fit(model, train_loader)
    
    print(f"Training completed. Best checkpoint: {checkpoint_callback.best_model_path}")
    
    return checkpoint_callback.best_model_path